{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9057601,"sourceType":"datasetVersion","datasetId":5461696,"isSourceIdPinned":false},{"sourceId":692257,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":524874,"modelId":538915},{"sourceId":692260,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":524877,"modelId":538918},{"sourceId":692671,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":525203,"modelId":539243}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# USING ENSEMBLE KNOWLEDGE DISTILLATION TO TRAIN THE STUDENT MODEL\n\nimport subprocess, sys\ndef install(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\ntry:\n    import timm\nexcept ImportError:\n    install(\"timm\")\n    import timm\n\n# IMPORTS\nimport os\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\n\n# CONFIG\nDATASET_DIR = Path(\"/kaggle/input/rice-disease-dataset/Rice_Leaf_AUG\")\n\nTEACHER_PATHS = {\n    \"resnet\": \"/kaggle/input/teacher-weights/pytorch/default/1/best_model_resnet18.pth\",\n    \"mobilenet\": \"/kaggle/input/teacher-weights/pytorch/default/1/best_model_mobilenet.pth\",\n    \"densenet\": \"/kaggle/input/teacher-weights/pytorch/default/1/best_model_densenet.pth\",\n}\n\nNUM_CLASSES = 6\nIMG_SIZE = 224\nBATCH_SIZE = 32\nEPOCHS = 25\nLR = 3e-4\nALPHA = 0.8\nTEMPERATURE = 4.0\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(\"=\" * 60)\nprint(f\"ENSEMBLE KD TRAINING ON {DEVICE}\")\nprint(\"=\" * 60)\n\n# ============================================================\n# SEED\n# ============================================================\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\n\n# DATASET PREPARATION\nfilepaths, labels = [], []\n\nfor cls in sorted(os.listdir(DATASET_DIR)):\n    cls_dir = DATASET_DIR / cls\n    if cls_dir.is_dir():\n        for img in os.listdir(cls_dir):\n            filepaths.append(str(cls_dir / img))\n            labels.append(cls)\n\ndf = pd.DataFrame({\"filepath\": filepaths, \"label\": labels})\n\nencoder = LabelEncoder()\ndf[\"label_encoded\"] = encoder.fit_transform(df[\"label\"])\nclass_names = encoder.classes_\n\nprint(\"Classes:\", class_names)\n\ntrain_df, val_df = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df[\"label\"],\n    random_state=42\n)\n\n\n# AUGMENTATIONS\ndef get_train_transforms():\n    return A.Compose([\n        A.Resize(IMG_SIZE, IMG_SIZE),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Rotate(limit=30, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.CLAHE(clip_limit=4.0, p=0.5),\n        A.CoarseDropout(\n            max_holes=8,\n            max_height=20,\n            max_width=20,\n            fill_value=0,\n            p=0.5\n        ),\n        A.Normalize(\n            mean=(0.485, 0.456, 0.406),\n            std=(0.229, 0.224, 0.225)\n        ),\n        ToTensorV2()\n    ])\n\ndef get_val_transforms():\n    return A.Compose([\n        A.Resize(IMG_SIZE, IMG_SIZE),\n        A.Normalize(\n            mean=(0.485, 0.456, 0.406),\n            std=(0.229, 0.224, 0.225)\n        ),\n        ToTensorV2()\n    ])\n\nclass RiceDataset(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df = df.reset_index(drop=True)\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = cv2.imread(row[\"filepath\"])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        label = row[\"label_encoded\"]\n\n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n\n        return img, torch.tensor(label, dtype=torch.long)\n\ntrain_loader = DataLoader(\n    RiceDataset(train_df, get_train_transforms()),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    RiceDataset(val_df, get_val_transforms()),\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\n\n# TEACHER MODELS \nclass RiceResNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = models.resnet18(weights=None)\n        num_ftrs = self.model.fc.in_features\n        self.model.fc = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(num_ftrs, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, NUM_CLASSES)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\nclass RiceMobileNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = models.mobilenet_v2(weights=None)\n        self.model.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(1280, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, NUM_CLASSES)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\nclass RiceDenseNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = models.densenet121(weights=None)\n\n        num_ftrs = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(num_ftrs, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, NUM_CLASSES)\n        )\n\n    def forward(self, x):\n        return self.backbone(x)\n\ndef clean_state_dict(state_dict):\n    new_state = {}\n    for k, v in state_dict.items():\n        if k.startswith(\"module.\"):\n            k = k.replace(\"module.\", \"\")\n        if k.startswith(\"model.\"):\n            k = k.replace(\"model.\", \"\")\n        new_state[k] = v\n    return new_state\n\n\n\n# LOAD TEACHERS\ndef load_teacher(model, path, name):\n    print(f\"Loading {name} teacher...\")\n    state = torch.load(path, map_location=DEVICE)\n    state = clean_state_dict(state)\n    if isinstance(model, RiceDenseNet):\n        model.backbone.load_state_dict(state, strict=True)\n    else:\n        model.model.load_state_dict(state, strict=True)\n\n    model.to(DEVICE)\n    model.eval()\n    for p in model.parameters():\n        p.requires_grad = False\n\n    print(f\"{name} loaded successfully\")\n    return model\n\n\nteachers = [\n    load_teacher(RiceResNet(), TEACHER_PATHS[\"resnet\"], \"ResNet18\"),\n    load_teacher(RiceMobileNet(), TEACHER_PATHS[\"mobilenet\"], \"MobileNetV2\"),\n    load_teacher(RiceDenseNet(), TEACHER_PATHS[\"densenet\"], \"DenseNet121\"),\n]\n\nprint(f\"Loaded {len(teachers)} teachers successfully\")\n\n\n# STUDENT MODEL\nclass StudentModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = timm.create_model(\n            \"mobilenetv3_small_050\",\n            pretrained=True\n        )\n        num_ftrs = self.model.classifier.in_features\n        self.model.classifier = nn.Sequential(\n            nn.Linear(num_ftrs, 256),\n            nn.Hardswish(),\n            nn.Dropout(0.1),\n            nn.Linear(256, 128),\n            nn.Hardswish(),\n            nn.Dropout(0.1),\n            nn.Linear(128, NUM_CLASSES)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\nstudent = StudentModel().to(DEVICE)\n\n\n# KD LOSS\nclass EnsembleKDLoss(nn.Module):\n    def __init__(self, alpha=0.8, T=4.0):\n        super().__init__()\n        self.alpha = alpha\n        self.T = T\n        self.ce = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n    def forward(self, s_logits, targets, t_logits):\n        ce_loss = self.ce(s_logits, targets)\n        kd_loss = F.kl_div(\n            F.log_softmax(s_logits / self.T, dim=1),\n            F.softmax(t_logits / self.T, dim=1),\n            reduction=\"batchmean\"\n        ) * (self.T ** 2)\n\n        return self.alpha * kd_loss + (1 - self.alpha) * ce_loss\n\ncriterion = EnsembleKDLoss(ALPHA, TEMPERATURE)\noptimizer = optim.AdamW(student.parameters(), lr=LR, weight_decay=1e-5)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\nscaler = torch.cuda.amp.GradScaler()\n\n\n# TRAIN / EVAL\ndef train_epoch(model, loader):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n\n    for imgs, labels in tqdm(loader, leave=False):\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n\n        with torch.no_grad():\n            t_logits = torch.mean(\n                torch.stack([t(imgs) for t in teachers]),\n                dim=0\n            )\n\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n            s_logits = model(imgs)\n            loss = criterion(s_logits, labels, t_logits)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item() * imgs.size(0)\n        preds = s_logits.argmax(1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    return total_loss / total, correct / total\n\ndef evaluate(model, loader):\n    model.eval()\n    correct, total = 0, 0\n    all_preds, all_labels = [], []\n\n    with torch.no_grad():\n        for imgs, labels in loader:\n            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n            logits = model(imgs)\n            preds = logits.argmax(1)\n\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    return correct / total, all_labels, all_preds\n\n\n# TRAIN LOOP\nbest_acc = 0.0\nprint(\"\\nStarting Knowledge Distillation...\\n\")\n\nfor epoch in range(EPOCHS):\n    train_loss, train_acc = train_epoch(student, train_loader)\n    val_acc, y_true, y_pred = evaluate(student, val_loader)\n    scheduler.step()\n\n    print(\n        f\"Epoch [{epoch+1}/{EPOCHS}] \"\n        f\"Loss: {train_loss:.4f} \"\n        f\"Train Acc: {train_acc:.4f} \"\n        f\"Val Acc: {val_acc:.4f}\"\n    )\n\n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save(student.state_dict(), \"best_student.pth\")\n        print(\">> Best Student Saved\")\n\n\n# FINAL REPORT\nprint(\"=\" * 40)\nprint(f\"BEST VALIDATION ACCURACY: {best_acc:.4f}\")\n\ncm = confusion_matrix(y_true, y_pred)\nprint(\"\\nConfusion Matrix:\\n\", cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:00:46.118380Z","iopub.execute_input":"2025-12-19T16:00:46.119110Z","iopub.status.idle":"2025-12-19T16:12:35.624066Z","shell.execute_reply.started":"2025-12-19T16:00:46.119081Z","shell.execute_reply":"2025-12-19T16:12:35.623269Z"}},"outputs":[{"name":"stdout","text":"============================================================\nENSEMBLE KD TRAINING ON cuda\n============================================================\nClasses: ['Bacterial Leaf Blight' 'Brown Spot' 'Healthy Rice Leaf' 'Leaf Blast'\n 'Leaf scald' 'Sheath Blight']\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/3590033457.py:117: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n  A.CoarseDropout(\n","output_type":"stream"},{"name":"stdout","text":"Loading ResNet18 teacher...\nResNet18 loaded successfully\nLoading MobileNetV2 teacher...\nMobileNetV2 loaded successfully\nLoading DenseNet121 teacher...\nDenseNet121 loaded successfully\nLoaded 3 teachers successfully\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/3590033457.py:322: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"\nStarting Knowledge Distillation...\n\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/96 [00:00<?, ?it/s]/tmp/ipykernel_55/3590033457.py:342: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [1/25] Loss: 1.1401 Train Acc: 0.4894 Val Acc: 0.7102\n>> Best Student Saved\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [2/25] Loss: 0.6986 Train Acc: 0.7339 Val Acc: 0.7911\n>> Best Student Saved\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [3/25] Loss: 0.5640 Train Acc: 0.7901 Val Acc: 0.8394\n>> Best Student Saved\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [4/25] Loss: 0.4892 Train Acc: 0.8341 Val Acc: 0.8695\n>> Best Student Saved\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [5/25] Loss: 0.4295 Train Acc: 0.8563 Val Acc: 0.8956\n>> Best Student Saved\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [6/25] Loss: 0.3817 Train Acc: 0.8821 Val Acc: 0.9151\n>> Best Student Saved\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [7/25] Loss: 0.3449 Train Acc: 0.8985 Val Acc: 0.9191\n>> Best Student Saved\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [8/25] Loss: 0.3264 Train Acc: 0.9089 Val Acc: 0.9191\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [9/25] Loss: 0.3033 Train Acc: 0.9177 Val Acc: 0.9347\n>> Best Student Saved\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [10/25] Loss: 0.2854 Train Acc: 0.9243 Val Acc: 0.9321\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [11/25] Loss: 0.2782 Train Acc: 0.9259 Val Acc: 0.9426\n>> Best Student Saved\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [12/25] Loss: 0.2653 Train Acc: 0.9341 Val Acc: 0.9399\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [13/25] Loss: 0.2559 Train Acc: 0.9396 Val Acc: 0.9517\n>> Best Student Saved\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [14/25] Loss: 0.2466 Train Acc: 0.9461 Val Acc: 0.9452\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [15/25] Loss: 0.2375 Train Acc: 0.9474 Val Acc: 0.9504\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [16/25] Loss: 0.2300 Train Acc: 0.9507 Val Acc: 0.9517\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [17/25] Loss: 0.2255 Train Acc: 0.9543 Val Acc: 0.9569\n>> Best Student Saved\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [18/25] Loss: 0.2238 Train Acc: 0.9585 Val Acc: 0.9530\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [19/25] Loss: 0.2200 Train Acc: 0.9549 Val Acc: 0.9543\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [20/25] Loss: 0.2195 Train Acc: 0.9572 Val Acc: 0.9543\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [21/25] Loss: 0.2196 Train Acc: 0.9549 Val Acc: 0.9556\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [22/25] Loss: 0.2161 Train Acc: 0.9582 Val Acc: 0.9608\n>> Best Student Saved\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [23/25] Loss: 0.2135 Train Acc: 0.9576 Val Acc: 0.9582\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [24/25] Loss: 0.2136 Train Acc: 0.9611 Val Acc: 0.9608\n","output_type":"stream"},{"name":"stderr","text":"                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [25/25] Loss: 0.2153 Train Acc: 0.9549 Val Acc: 0.9569\n========================================\nBEST VALIDATION ACCURACY: 0.9608\n\nConfusion Matrix:\n [[124   0   0   0   3   0]\n [  2 119   0   4   3   1]\n [  0   0 131   0   0   0]\n [  4   3   0 118   1   1]\n [  0   1   0   3 121   1]\n [  3   1   1   1   0 120]]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"pip install torch torchvision onnx onnxruntime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T18:29:56.769480Z","iopub.execute_input":"2025-12-19T18:29:56.769779Z","iopub.status.idle":"2025-12-19T18:30:02.383950Z","shell.execute_reply.started":"2025-12-19T18:29:56.769754Z","shell.execute_reply":"2025-12-19T18:30:02.383089Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\nRequirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.20.0)\nCollecting onnxruntime\n  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\nRequirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\nCollecting coloredlogs (from onnxruntime)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.9.23)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\nDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# DOING QAT for QUANTIZATION AND CONVERT TO INT8 ONNX FORMAT\n\nimport os\nimport copy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.quantization.quantize_fx as quantize_fx\nimport onnx\nimport onnxruntime\nimport numpy as np\nimport timm\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom tqdm import tqdm\n\n# 1. CONFIGURATION\nMODEL_INPUT_PATH = \"/kaggle/working/best_student.pth\"\nONNX_OUTPUT_PATH = \"/kaggle/working/student_int8_qat_optimized.onnx\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# High-Performance Hyperparameters\nTOTAL_EPOCHS = 50     \nFREEZE_EPOCH = 10      \nSTART_LR = 1e-4         \nMIN_LR = 1e-6           \n\nprint(f\"Running Optimized QAT on: {DEVICE}\")\nprint(f\"Strategy: 1-{FREEZE_EPOCH} Learn Ranges | {FREEZE_EPOCH+1}-{TOTAL_EPOCHS} Fine-tune Weights\")\n\n\n# 2. DEFINE MODEL\nclass StudentModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = timm.create_model(\"mobilenetv3_small_050\", pretrained=False)\n        num_ftrs = self.model.classifier.in_features\n        self.model.classifier = nn.Sequential(\n            nn.Linear(num_ftrs, 256),\n            nn.Hardswish(),\n            nn.Dropout(0.1),\n            nn.Linear(256, 128),\n            nn.Hardswish(),\n            nn.Dropout(0.1),\n            nn.Linear(128, 6)\n        )\n    def forward(self, x):\n        return self.model(x)\n\n\n# 3. PREPARE FOR QAT\nprint(\"\\n[1/6] Loading & Fusing Model...\")\nmodel = StudentModel().to(DEVICE)\nmodel.load_state_dict(torch.load(MODEL_INPUT_PATH, map_location=DEVICE))\nqconfig_dict = {\"\": torch.quantization.get_default_qat_qconfig('qnnpack')}\nmodel.train() \n\nexample_input = torch.randn(1, 3, 224, 224).to(DEVICE)\nmodel_prepared = quantize_fx.prepare_qat_fx(model, qconfig_dict, example_inputs=example_input)\nprint(\"      Model prepared.\")\n\n\n# 4. OPTIMIZED TRAINING LOOP\nprint(f\"\\n[2/6] Starting QAT Process...\")\n\noptimizer = optim.Adam(model_prepared.parameters(), lr=START_LR)\nscheduler = CosineAnnealingLR(optimizer, T_max=TOTAL_EPOCHS, eta_min=MIN_LR)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\nbest_acc = 0.0\nbest_model_weights = copy.deepcopy(model_prepared.state_dict())\n\nfor epoch in range(TOTAL_EPOCHS):\n    model_prepared.train()\n    \n    # === FREEZE OBSERVERS ===\n    if epoch == FREEZE_EPOCH:\n        print(f\"\\n      >>> STABILIZATION PHASE: Freezing Observers & BN Stats <<<\")\n        model_prepared.apply(torch.quantization.disable_observer)\n        model_prepared.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n    \n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{TOTAL_EPOCHS}\")\n    for imgs, labels in pbar:\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n        \n        optimizer.zero_grad()\n        outputs = model_prepared(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        # Stats\n        running_loss += loss.item() * imgs.size(0)\n        preds = outputs.argmax(1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n        \n        pbar.set_postfix({'loss': loss.item(), 'acc': f\"{correct/total:.2%}\"})\n\n    scheduler.step()\n    \n    # Save best\n    epoch_acc = correct / total\n    if epoch_acc > best_acc:\n        best_acc = epoch_acc\n        best_model_weights = copy.deepcopy(model_prepared.state_dict())\n\nprint(f\"\\n      Training Complete. Best Accuracy: {best_acc*100:.2f}%\")\nmodel_prepared.load_state_dict(best_model_weights)\n\n\n# 5. CONVERSION & EXPORT\nprint(\"\\n[3/6] Moving to CPU for Conversion...\")\nmodel_prepared.to(\"cpu\")\nmodel_prepared.eval()\n\nprint(\"      Converting to INT8...\")\nquantized_model = quantize_fx.convert_fx(model_prepared)\n\nprint(f\"\\n[4/6] Exporting to ONNX: {ONNX_OUTPUT_PATH}\")\ndummy_input_cpu = torch.randn(1, 3, 224, 224)\n\ntorch.onnx.export(\n    quantized_model,\n    dummy_input_cpu,\n    ONNX_OUTPUT_PATH,\n    input_names=['input'],\n    output_names=['output'],\n    opset_version=13,\n    do_constant_folding=True,\n    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}} \n)\n\n\n# 6. FINAL VALIDATION\nprint(\"\\n[5/6] Final Verification on Validation Set...\")\nsession = onnxruntime.InferenceSession(ONNX_OUTPUT_PATH, providers=['CPUExecutionProvider'])\ninput_name = session.get_inputs()[0].name\noutput_name = session.get_outputs()[0].name\n\ncorrect = 0\ntotal = 0\n\nfor imgs, labels in tqdm(val_loader, desc=\"Testing ONNX\"):\n    img_numpy = imgs.numpy()\n    ort_outs = session.run([output_name], {input_name: img_numpy})\n    preds = np.argmax(ort_outs[0], axis=1)\n    correct += (preds == labels.numpy()).sum()\n    total += labels.size(0)\n\nfinal_acc = correct / total\n\nprint(\"\\n\" + \"=\"*40)\nprint(f\"FINAL OPTIMIZED INT8 ACCURACY: {final_acc*100:.2f}%\")\nprint(\"=\"*40)\n\nif final_acc > 0.90:\n    print(\"✅ MISSION ACCOMPLISHED: 90%+ INT8 Model Created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T17:03:28.394384Z","iopub.execute_input":"2025-12-19T17:03:28.394755Z","iopub.status.idle":"2025-12-19T17:18:24.218843Z","shell.execute_reply.started":"2025-12-19T17:03:28.394722Z","shell.execute_reply":"2025-12-19T17:18:24.217911Z"}},"outputs":[{"name":"stdout","text":"Running Optimized QAT on: cuda\nStrategy: 1-10 Learn Ranges | 11-50 Fine-tune Weights\n\n[1/6] Loading & Fusing Model...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/2152863518.py:63: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \nFor migrations of users: \n1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \nsee https://github.com/pytorch/ao/issues/2259 for more details\n  model_prepared = quantize_fx.prepare_qat_fx(model, qconfig_dict, example_inputs=example_input)\n/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n  prepared = prepare(\n/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/fx/utils.py:927: UserWarning: QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize for fixed qparams ops, ignoring QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=False){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x79820d489800>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x79820d489800>}).\nPlease use torch.ao.quantization.get_default_qconfig_mapping or torch.ao.quantization.get_default_qat_qconfig_mapping. Example:\n    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n    model = prepare_fx(model, qconfig_mapping, example_inputs)\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/fx/utils.py:927: UserWarning: QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize for fixed qparams ops, ignoring QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=False){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x79820d488d60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x79820d488d60>}).\nPlease use torch.ao.quantization.get_default_qconfig_mapping or torch.ao.quantization.get_default_qat_qconfig_mapping. Example:\n    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n    model = prepare_fx(model, qconfig_mapping, example_inputs)\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/fx/utils.py:927: UserWarning: QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize for fixed qparams ops, ignoring QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=False){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x79820d488360>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x79820d488360>}).\nPlease use torch.ao.quantization.get_default_qconfig_mapping or torch.ao.quantization.get_default_qat_qconfig_mapping. Example:\n    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n    model = prepare_fx(model, qconfig_mapping, example_inputs)\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/fx/utils.py:927: UserWarning: QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize for fixed qparams ops, ignoring QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=False){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x79820d477880>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x79820d477880>}).\nPlease use torch.ao.quantization.get_default_qconfig_mapping or torch.ao.quantization.get_default_qat_qconfig_mapping. Example:\n    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n    model = prepare_fx(model, qconfig_mapping, example_inputs)\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/fx/utils.py:927: UserWarning: QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize for fixed qparams ops, ignoring QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=False){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x79820d474540>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x79820d474540>}).\nPlease use torch.ao.quantization.get_default_qconfig_mapping or torch.ao.quantization.get_default_qat_qconfig_mapping. Example:\n    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n    model = prepare_fx(model, qconfig_mapping, example_inputs)\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/fx/utils.py:927: UserWarning: QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize for fixed qparams ops, ignoring QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=False){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x79820d475ee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x79820d475ee0>}).\nPlease use torch.ao.quantization.get_default_qconfig_mapping or torch.ao.quantization.get_default_qat_qconfig_mapping. Example:\n    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n    model = prepare_fx(model, qconfig_mapping, example_inputs)\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/fx/utils.py:927: UserWarning: QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize for fixed qparams ops, ignoring QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=False){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7981a08818a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7981a08818a0>}).\nPlease use torch.ao.quantization.get_default_qconfig_mapping or torch.ao.quantization.get_default_qat_qconfig_mapping. Example:\n    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n    model = prepare_fx(model, qconfig_mapping, example_inputs)\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/fx/utils.py:927: UserWarning: QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize for fixed qparams ops, ignoring QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=False){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7981a0883740>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7981a0883740>}).\nPlease use torch.ao.quantization.get_default_qconfig_mapping or torch.ao.quantization.get_default_qat_qconfig_mapping. Example:\n    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n    model = prepare_fx(model, qconfig_mapping, example_inputs)\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/fx/utils.py:927: UserWarning: QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize for fixed qparams ops, ignoring QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=False){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x79820d4cfe20>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x79820d4cfe20>}).\nPlease use torch.ao.quantization.get_default_qconfig_mapping or torch.ao.quantization.get_default_qat_qconfig_mapping. Example:\n    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n    model = prepare_fx(model, qconfig_mapping, example_inputs)\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"      Model prepared.\n\n[2/6] Starting QAT Process...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 96/96 [00:17<00:00,  5.48it/s, loss=1.03, acc=77.21%] \nEpoch 2/50: 100%|██████████| 96/96 [00:17<00:00,  5.47it/s, loss=0.814, acc=82.53%]\nEpoch 3/50: 100%|██████████| 96/96 [00:17<00:00,  5.41it/s, loss=0.844, acc=84.69%]\nEpoch 4/50: 100%|██████████| 96/96 [00:17<00:00,  5.43it/s, loss=0.632, acc=86.19%]\nEpoch 5/50: 100%|██████████| 96/96 [00:17<00:00,  5.37it/s, loss=0.595, acc=87.23%]\nEpoch 6/50: 100%|██████████| 96/96 [00:17<00:00,  5.34it/s, loss=0.809, acc=85.93%]\nEpoch 7/50: 100%|██████████| 96/96 [00:17<00:00,  5.50it/s, loss=0.584, acc=89.00%]\nEpoch 8/50: 100%|██████████| 96/96 [00:17<00:00,  5.48it/s, loss=0.507, acc=89.85%]\nEpoch 9/50: 100%|██████████| 96/96 [00:17<00:00,  5.58it/s, loss=0.732, acc=90.60%]\nEpoch 10/50: 100%|██████████| 96/96 [00:18<00:00,  5.31it/s, loss=0.777, acc=91.02%]\n","output_type":"stream"},{"name":"stdout","text":"\n      >>> STABILIZATION PHASE: Freezing Observers & BN Stats <<<\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50: 100%|██████████| 96/96 [00:17<00:00,  5.37it/s, loss=0.579, acc=91.71%]\nEpoch 12/50: 100%|██████████| 96/96 [00:17<00:00,  5.52it/s, loss=0.598, acc=91.90%]\nEpoch 13/50: 100%|██████████| 96/96 [00:18<00:00,  5.31it/s, loss=0.631, acc=92.62%]\nEpoch 14/50: 100%|██████████| 96/96 [00:18<00:00,  5.24it/s, loss=0.778, acc=92.46%]\nEpoch 15/50: 100%|██████████| 96/96 [00:17<00:00,  5.45it/s, loss=0.739, acc=92.88%]\nEpoch 16/50: 100%|██████████| 96/96 [00:17<00:00,  5.45it/s, loss=0.728, acc=92.92%]\nEpoch 17/50: 100%|██████████| 96/96 [00:17<00:00,  5.37it/s, loss=0.529, acc=93.93%]\nEpoch 18/50: 100%|██████████| 96/96 [00:17<00:00,  5.45it/s, loss=0.692, acc=94.06%]\nEpoch 19/50: 100%|██████████| 96/96 [00:17<00:00,  5.40it/s, loss=0.86, acc=95.00%] \nEpoch 20/50: 100%|██████████| 96/96 [00:16<00:00,  5.67it/s, loss=0.67, acc=94.12%] \nEpoch 21/50: 100%|██████████| 96/96 [00:16<00:00,  5.66it/s, loss=0.489, acc=94.32%]\nEpoch 22/50: 100%|██████████| 96/96 [00:17<00:00,  5.52it/s, loss=0.574, acc=94.97%]\nEpoch 23/50: 100%|██████████| 96/96 [00:17<00:00,  5.41it/s, loss=0.639, acc=94.87%]\nEpoch 24/50: 100%|██████████| 96/96 [00:17<00:00,  5.51it/s, loss=0.663, acc=95.10%]\nEpoch 25/50: 100%|██████████| 96/96 [00:17<00:00,  5.43it/s, loss=0.589, acc=94.87%]\nEpoch 26/50: 100%|██████████| 96/96 [00:17<00:00,  5.40it/s, loss=0.603, acc=95.36%]\nEpoch 27/50: 100%|██████████| 96/96 [00:18<00:00,  5.32it/s, loss=0.537, acc=95.59%]\nEpoch 28/50: 100%|██████████| 96/96 [00:17<00:00,  5.61it/s, loss=0.615, acc=95.53%]\nEpoch 29/50: 100%|██████████| 96/96 [00:17<00:00,  5.52it/s, loss=0.551, acc=96.15%]\nEpoch 30/50: 100%|██████████| 96/96 [00:17<00:00,  5.57it/s, loss=0.5, acc=95.92%]  \nEpoch 31/50: 100%|██████████| 96/96 [00:17<00:00,  5.48it/s, loss=0.479, acc=95.49%]\nEpoch 32/50: 100%|██████████| 96/96 [00:17<00:00,  5.58it/s, loss=0.46, acc=96.47%] \nEpoch 33/50: 100%|██████████| 96/96 [00:17<00:00,  5.52it/s, loss=0.478, acc=96.74%]\nEpoch 34/50: 100%|██████████| 96/96 [00:17<00:00,  5.46it/s, loss=0.531, acc=96.31%]\nEpoch 35/50: 100%|██████████| 96/96 [00:17<00:00,  5.64it/s, loss=0.519, acc=96.93%]\nEpoch 36/50: 100%|██████████| 96/96 [00:17<00:00,  5.47it/s, loss=0.597, acc=96.74%]\nEpoch 37/50: 100%|██████████| 96/96 [00:17<00:00,  5.62it/s, loss=0.47, acc=96.47%] \nEpoch 38/50: 100%|██████████| 96/96 [00:17<00:00,  5.49it/s, loss=0.513, acc=96.47%]\nEpoch 39/50: 100%|██████████| 96/96 [00:17<00:00,  5.44it/s, loss=0.489, acc=96.34%]\nEpoch 40/50: 100%|██████████| 96/96 [00:17<00:00,  5.43it/s, loss=0.55, acc=97.29%] \nEpoch 41/50: 100%|██████████| 96/96 [00:17<00:00,  5.62it/s, loss=0.508, acc=97.19%]\nEpoch 42/50: 100%|██████████| 96/96 [00:17<00:00,  5.54it/s, loss=0.669, acc=96.44%]\nEpoch 43/50: 100%|██████████| 96/96 [00:18<00:00,  5.26it/s, loss=0.53, acc=97.19%] \nEpoch 44/50: 100%|██████████| 96/96 [00:17<00:00,  5.34it/s, loss=0.587, acc=96.77%]\nEpoch 45/50: 100%|██████████| 96/96 [00:17<00:00,  5.58it/s, loss=0.575, acc=96.54%]\nEpoch 46/50: 100%|██████████| 96/96 [00:17<00:00,  5.45it/s, loss=0.532, acc=97.19%]\nEpoch 47/50: 100%|██████████| 96/96 [00:18<00:00,  5.32it/s, loss=0.507, acc=96.87%]\nEpoch 48/50: 100%|██████████| 96/96 [00:17<00:00,  5.34it/s, loss=0.53, acc=97.06%] \nEpoch 49/50: 100%|██████████| 96/96 [00:17<00:00,  5.56it/s, loss=0.631, acc=96.77%]\nEpoch 50/50: 100%|██████████| 96/96 [00:17<00:00,  5.59it/s, loss=0.493, acc=96.90%]\n/tmp/ipykernel_55/2152863518.py:134: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \nFor migrations of users: \n1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \nsee https://github.com/pytorch/ao/issues/2259 for more details\n  quantized_model = quantize_fx.convert_fx(model_prepared)\n","output_type":"stream"},{"name":"stdout","text":"\n      Training Complete. Best Accuracy: 97.29%\n\n[3/6] Moving to CPU for Conversion...\n      Converting to INT8...\n\n[4/6] Exporting to ONNX: /kaggle/working/student_int8_qat_optimized.onnx\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/2152863518.py:139: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n  torch.onnx.export(\n/usr/local/lib/python3.12/dist-packages/torch/onnx/symbolic_helper.py:1460: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'batch_norm' is set to train=True. Exporting with train=True.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n[5/6] Final Verification on Validation Set...\n","output_type":"stream"},{"name":"stderr","text":"Testing ONNX: 100%|██████████| 24/24 [00:07<00:00,  3.33it/s]","output_type":"stream"},{"name":"stdout","text":"\n========================================\nFINAL OPTIMIZED INT8 ACCURACY: 91.25%\n========================================\n✅ MISSION ACCOMPLISHED: 90%+ INT8 Model Created.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# MODEL ANALYSIS\n\nimport os\nimport time\nimport numpy as np\nimport onnx\nimport onnxruntime as ort\nfrom onnx import numpy_helper\n\n# 1. SETUP\nONNX_MODEL_PATH = \"/kaggle/input/int8-qat-optimized-student/pytorch/default/1/student_int8_qat_optimized.onnx\"\nINPUT_SHAPE = (1, 3, 224, 224)\n\nif not os.path.exists(ONNX_MODEL_PATH):\n    raise FileNotFoundError(f\"Model not found at {ONNX_MODEL_PATH}\")\n\nprint(f\"Analyzing Model: {ONNX_MODEL_PATH}...\\n\")\n\n\n# 2. FILE SIZE\nsize_bytes = os.path.getsize(ONNX_MODEL_PATH)\nsize_mb = size_bytes / (1024 * 1024)\n\n\n# 3. NUMBER OF PARAMETERS\nmodel_onnx = onnx.load(ONNX_MODEL_PATH)\ntotal_params = 0\nfor initializer in model_onnx.graph.initializer:\n    dims = initializer.dims\n    if dims:\n        total_params += np.prod(dims)\n\n# 4. INFERENCE TIME (LATENCY)\nsession = ort.InferenceSession(ONNX_MODEL_PATH, providers=['CPUExecutionProvider'])\ninput_name = session.get_inputs()[0].name\noutput_name = session.get_outputs()[0].name\n\ndummy_input = np.random.randn(*INPUT_SHAPE).astype(np.float32)\n\n\nprint(\"Warming up inference engine...\")\nfor _ in range(50):\n    session.run([output_name], {input_name: dummy_input})\n\n# Measurement Loop\niterations = 1000\nprint(f\"Running {iterations} iterations for timing...\")\n\nlatencies = []\nfor _ in range(iterations):\n    start_time = time.time()\n    session.run([output_name], {input_name: dummy_input})\n    end_time = time.time()\n    latencies.append((end_time - start_time) * 1000)\n\navg_latency = np.mean(latencies)\nfps = 1000 / avg_latency\n\n# 5. FINAL REPORT\nprint(\"\\n\" + \"=\"*50)\nprint(f\"📊 FINAL INT8 MODEL METRICS\")\nprint(\"=\"*50)\nprint(f\"💾 Model File Size:       {size_mb:.2f} MB\")\nprint(f\"🧠 Total Parameters:      {total_params:,}\")\nprint(f\"⏱️ Avg Inference Time:    {avg_latency:.4f} ms per image (CPU)\")\nprint(f\"🚀 Throughput:            {fps:.2f} FPS\")\nprint(\"=\"*50)\n\n# Comparison Context\nprint(\"\\nComparison Context:\")\nprint(f\"- Float32 Size Estimate:  {size_mb * 4:.2f} MB (approx)\")\nprint(f\"- Size Reduction:         {((size_mb*4 - size_mb)/(size_mb*4))*100:.0f}% smaller\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T18:31:41.750102Z","iopub.execute_input":"2025-12-19T18:31:41.750404Z","iopub.status.idle":"2025-12-19T18:31:46.568880Z","shell.execute_reply.started":"2025-12-19T18:31:41.750379Z","shell.execute_reply":"2025-12-19T18:31:46.568116Z"}},"outputs":[{"name":"stdout","text":"Analyzing Model: /kaggle/input/int8-qat-optimized-student/pytorch/default/1/student_int8_qat_optimized.onnx...\n\nWarming up inference engine...\nRunning 1000 iterations for timing...\n\n==================================================\n📊 FINAL INT8 MODEL METRICS\n==================================================\n💾 Model File Size:       1.10 MB\n🧠 Total Parameters:      13,812\n⏱️ Avg Inference Time:    4.4274 ms per image (CPU)\n🚀 Throughput:            225.87 FPS\n==================================================\n\nComparison Context:\n- Float32 Size Estimate:  4.40 MB (approx)\n- Size Reduction:         75% smaller\n","output_type":"stream"}],"execution_count":5}]}
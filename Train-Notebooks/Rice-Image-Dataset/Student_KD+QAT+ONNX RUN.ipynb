{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14167666,"sourceType":"datasetVersion","datasetId":9030793},{"sourceId":689810,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":522862,"modelId":536861},{"sourceId":690263,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":523262,"modelId":537268},{"sourceId":690827,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":523726,"modelId":537748},{"sourceId":690829,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":523727,"modelId":537749}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# TRAINING OF FP32 KNOWLEDGE DISTILLATION STUDENT MODEL","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensemble Knowledge Distillation Training Script (FINAL, READY TO RUN)\n# Teachers: ResNet18, MobileNetV2, DenseNet121\n# Student: MobileNetV3-Small (width=0.25)\n# Dataset: Rice Image Dataset (5 classes, ~75k images)\n# Platform: Kaggle (GPU)\n\nimport os, time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom torchvision.datasets import ImageFolder\n\n# Config\nFULL_DATA_DIR = \"/kaggle/input/rice-image-dataset/Rice_Image_Dataset\"\nTEACHER_WEIGHTS_DIR = \"/kaggle/input/teacher-weights/pytorch/default/1\"\nNUM_CLASSES = 5\nIMG_SIZE = 224\nBATCH_SIZE = 64\nEPOCHS = 25\nLR = 3e-4\nWEIGHT_DECAY = 1e-4\nALPHA = 0.8\nTEMPERATURE = 4.0\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Logging\nprint(\"=\"*60)\nprint(\"Starting Ensemble Knowledge Distillation Training\")\nprint(\"Device:\", DEVICE)\nprint(\"=\"*60)\n\n# Augementations\ntrain_tfms = transforms.Compose([\n    transforms.Resize((IMG_SIZE + 32, IMG_SIZE + 32)),\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.RandomVerticalFlip(0.2),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nval_tfms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Data Split\nprint(\"Building dataframe for fast stratified split (no ImageFolder scan)...\")\nfrom pathlib import Path\nimport pandas as pd\nfrom PIL import Image\n\nroot = Path(FULL_DATA_DIR)\nclasses = sorted([d.name for d in root.iterdir() if d.is_dir()])\nclass_to_idx = {c: i for i, c in enumerate(classes)}\n\nimage_paths, labels = [], []\nfor cls in classes:\n    for img_path in (root / cls).glob(\"*.*\"):\n        image_paths.append(str(img_path))\n        labels.append(cls)\n\ndf = pd.DataFrame({\"filepath\": image_paths, \"label\": labels})\nprint(f\"Total images found: {len(df)}\")\n\ntrain_df, val_df = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df[\"label\"],\n    random_state=42\n)\n\nclass RiceDataset(torch.utils.data.Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row[\"filepath\"]).convert(\"RGB\")\n        label = class_to_idx[row[\"label\"]]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\ntrain_ds = RiceDataset(train_df, transform=train_tfms)\nval_ds   = RiceDataset(val_df, transform=val_tfms)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\nval_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n\nprint(f\"Train samples: {len(train_ds)} | Val samples: {len(val_ds)}\")\nprint(\"Class mapping:\", class_to_idx)\n\n# Load Teachers\ndef load_teacher(model, weight_path, name):\n    print(f\"Loading teacher: {name}\")\n    checkpoint = torch.load(weight_path, map_location=\"cpu\")\n\n    # Handle both raw state_dict and full checkpoint formats\n    if isinstance(checkpoint, dict) and \"model_state\" in checkpoint:\n        state_dict = checkpoint[\"model_state\"]\n    else:\n        state_dict = checkpoint\n\n    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n    if len(missing) > 0:\n        print(f\"  [Info] Missing keys ignored: {len(missing)}\")\n    if len(unexpected) > 0:\n        print(f\"  [Info] Unexpected keys ignored: {len(unexpected)}\")\n\n    model.eval()\n    for p in model.parameters():\n        p.requires_grad = False\n    return model.to(DEVICE)\n\nprint(\"Loading teacher models...\")\nresnet18 = torchvision.models.resnet18(weights=None)\nresnet18.fc = nn.Linear(resnet18.fc.in_features, NUM_CLASSES)\nresnet18 = load_teacher(resnet18, os.path.join(TEACHER_WEIGHTS_DIR, \"best_resnet18_rice.pth\"), \"ResNet18\")\n\nmobilenetv2 = torchvision.models.mobilenet_v2(weights=None)\nmobilenetv2.classifier[1] = nn.Linear(mobilenetv2.classifier[1].in_features, NUM_CLASSES)\nmobilenetv2 = load_teacher(mobilenetv2, os.path.join(TEACHER_WEIGHTS_DIR, \"best_mobilenetv2_rice_scratch.pth\"), \"MobileNetV2\")\n\ndensenet121 = torchvision.models.densenet121(weights=None)\ndensenet121.classifier = nn.Linear(densenet121.classifier.in_features, NUM_CLASSES)\ndensenet121 = load_teacher(densenet121, os.path.join(TEACHER_WEIGHTS_DIR, \"best_densenet121_rice_scratch.pth\"), \"DenseNet121\")\n\nteachers = [resnet18, mobilenetv2, densenet121]\nprint(\"All teachers loaded and frozen.\")\n\n# Student Model\nprint(\"Initializing student model: MobileNetV3-Small 0.25x\")\nstudent = torchvision.models.mobilenet_v3_small(width_mult=0.25, weights=None)\nstudent.classifier[3] = nn.Linear(student.classifier[3].in_features, NUM_CLASSES)\nstudent = student.to(DEVICE)\n\n# Define the KD Loss\nclass EnsembleKDLoss(nn.Module):\n    def __init__(self, alpha=0.8, T=4.0):\n        super().__init__()\n        self.alpha = alpha\n        self.T = T\n        self.ce = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n    def forward(self, student_logits, targets, teacher_logits):\n        ce_loss = self.ce(student_logits, targets)\n        kd_loss = F.kl_div(\n            F.log_softmax(student_logits / self.T, dim=1),\n            F.softmax(teacher_logits / self.T, dim=1),\n            reduction=\"batchmean\"\n        ) * (self.T ** 2)\n        return self.alpha * kd_loss + (1 - self.alpha) * ce_loss\n\ncriterion = EnsembleKDLoss(ALPHA, TEMPERATURE)\noptimizer = AdamW(student.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nscheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n\n# Train/Eval Functions\ndef evaluate(model):\n    model.eval()\n    correct, total = 0, 0\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for x, y in val_loader:\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            logits = model(x)\n            preds = logits.argmax(1)\n            correct += (preds == y).sum().item()\n            total += y.size(0)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(y.cpu().numpy())\n    return 100. * correct / total, np.array(all_labels), np.array(all_preds)\n\nbest_acc = 0.0\nbest_preds, best_labels = None, None\n\nprint(\"\\nStarting training loop...\")\nfor epoch in range(EPOCHS):\n    start_time = time.time()\n    student.train()\n    correct, total, running_loss = 0, 0, 0.0\n\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n    for imgs, labels in pbar:\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n\n        with torch.no_grad():\n            logits_ens = sum(t(imgs) for t in teachers) / len(teachers)\n\n        outputs = student(imgs)\n        loss = criterion(outputs, labels, logits_ens)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        preds = outputs.argmax(1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n        running_loss += loss.item()\n\n        pbar.set_postfix(loss=running_loss/total, train_acc=100.*correct/total)\n\n    scheduler.step()\n    train_acc = 100. * correct / total\n    val_acc, labels_np, preds_np = evaluate(student)\n    epoch_time = time.time() - start_time\n\n    if val_acc > best_acc:\n        best_acc = val_acc\n        best_labels, best_preds = labels_np, preds_np\n        torch.save(student.state_dict(), \"best_student_mobilenetv3_025_kd.pth\")\n\n    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.2f}% | Val Acc={val_acc:.2f}% | Time={epoch_time:.1f}s\")\n\n# Final Report\nprint(\"\\nTraining completed.\")\nprint(\"Best Validation Accuracy: {:.2f}%\".format(best_acc))\n\n# Model stats\nnum_params = sum(p.numel() for p in student.parameters())\nmodel_size_mb = num_params * 4 / (1024 ** 2)\n\n# Inference speed\nstudent.eval()\ndummy = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)\nwith torch.no_grad():\n    t0 = time.time()\n    for _ in range(100):\n        _ = student(dummy)\n    t1 = time.time()\n\navg_inference_ms = (t1 - t0) / 100 * 1000\n\nprint(\"\\n===== STUDENT MODEL SUMMARY =====\")\nprint(\"Model: MobileNetV3-Small (0.25x)\")\nprint(f\"Parameters: {num_params:,}\")\nprint(f\"Model Size: {model_size_mb:.2f} MB\")\nprint(f\"Avg Inference Time: {avg_inference_ms:.2f} ms\")\n\n# Confusion Matrix\ncm = confusion_matrix(best_labels, best_preds)\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T07:17:22.212462Z","iopub.execute_input":"2025-12-17T07:17:22.212794Z","iopub.status.idle":"2025-12-17T08:56:35.510865Z","shell.execute_reply.started":"2025-12-17T07:17:22.212757Z","shell.execute_reply":"2025-12-17T08:56:35.510075Z"}},"outputs":[{"name":"stdout","text":"============================================================\nStarting Ensemble Knowledge Distillation Training\nDevice: cuda\n============================================================\nBuilding dataframe for fast stratified split (no ImageFolder scan)...\nTotal images found: 75000\nTrain samples: 60000 | Val samples: 15000\nClass mapping: {'Arborio': 0, 'Basmati': 1, 'Ipsala': 2, 'Jasmine': 3, 'Karacadag': 4}\nLoading teacher models...\nLoading teacher: ResNet18\n  [Info] Missing keys ignored: 102\n  [Info] Unexpected keys ignored: 122\nLoading teacher: MobileNetV2\nLoading teacher: DenseNet121\nAll teachers loaded and frozen.\nInitializing student model: MobileNetV3-Small 0.25x\n\nStarting training loop...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/25: 100%|██████████| 938/938 [03:33<00:00,  4.39it/s, loss=0.0387, train_acc=78.6]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Acc=78.64% | Val Acc=63.83% | Time=233.3s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/25: 100%|██████████| 938/938 [03:34<00:00,  4.37it/s, loss=0.0122, train_acc=92.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Acc=92.84% | Val Acc=93.17% | Time=233.7s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/25: 100%|██████████| 938/938 [03:34<00:00,  4.36it/s, loss=0.00842, train_acc=94.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Acc=94.34% | Val Acc=98.75% | Time=234.4s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/25: 100%|██████████| 938/938 [03:36<00:00,  4.33it/s, loss=0.00714, train_acc=94.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Acc=94.94% | Val Acc=94.73% | Time=236.4s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/25: 100%|██████████| 938/938 [03:38<00:00,  4.30it/s, loss=0.00647, train_acc=95.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Acc=95.16% | Val Acc=92.50% | Time=237.9s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/25: 100%|██████████| 938/938 [03:36<00:00,  4.32it/s, loss=0.0061, train_acc=95.3] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Acc=95.31% | Val Acc=96.84% | Time=236.8s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/25: 100%|██████████| 938/938 [03:39<00:00,  4.27it/s, loss=0.00575, train_acc=95.4]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Acc=95.44% | Val Acc=99.51% | Time=239.8s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/25: 100%|██████████| 938/938 [03:38<00:00,  4.30it/s, loss=0.00552, train_acc=95.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Acc=95.31% | Val Acc=64.71% | Time=237.9s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/25: 100%|██████████| 938/938 [03:38<00:00,  4.28it/s, loss=0.0053, train_acc=95.6] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Acc=95.64% | Val Acc=91.20% | Time=238.4s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/25: 100%|██████████| 938/938 [03:40<00:00,  4.26it/s, loss=0.00514, train_acc=95.5]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Acc=95.47% | Val Acc=99.36% | Time=239.3s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/25: 100%|██████████| 938/938 [03:38<00:00,  4.30it/s, loss=0.00504, train_acc=95.5]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Acc=95.50% | Val Acc=97.25% | Time=237.7s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/25: 100%|██████████| 938/938 [03:38<00:00,  4.28it/s, loss=0.00488, train_acc=95.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Acc=95.70% | Val Acc=99.63% | Time=238.3s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/25: 100%|██████████| 938/938 [03:42<00:00,  4.22it/s, loss=0.00482, train_acc=95.6]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Acc=95.59% | Val Acc=70.57% | Time=242.2s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/25: 100%|██████████| 938/938 [03:40<00:00,  4.25it/s, loss=0.00471, train_acc=95.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Acc=95.75% | Val Acc=99.23% | Time=240.7s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/25: 100%|██████████| 938/938 [03:36<00:00,  4.32it/s, loss=0.00468, train_acc=95.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Acc=95.72% | Val Acc=99.21% | Time=236.9s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/25: 100%|██████████| 938/938 [03:37<00:00,  4.31it/s, loss=0.00459, train_acc=95.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Acc=95.74% | Val Acc=99.43% | Time=237.5s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/25: 100%|██████████| 938/938 [03:41<00:00,  4.24it/s, loss=0.00453, train_acc=95.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Acc=95.74% | Val Acc=99.66% | Time=240.9s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/25: 100%|██████████| 938/938 [03:40<00:00,  4.25it/s, loss=0.00449, train_acc=95.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Acc=95.84% | Val Acc=97.97% | Time=240.0s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/25: 100%|██████████| 938/938 [03:38<00:00,  4.29it/s, loss=0.00449, train_acc=95.6]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Acc=95.64% | Val Acc=99.40% | Time=238.0s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/25: 100%|██████████| 938/938 [03:38<00:00,  4.30it/s, loss=0.00444, train_acc=95.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Acc=95.79% | Val Acc=98.85% | Time=237.9s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/25: 100%|██████████| 938/938 [03:40<00:00,  4.25it/s, loss=0.00442, train_acc=95.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Acc=95.84% | Val Acc=98.43% | Time=240.2s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/25: 100%|██████████| 938/938 [03:40<00:00,  4.25it/s, loss=0.00441, train_acc=95.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Acc=95.82% | Val Acc=97.74% | Time=240.5s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/25: 100%|██████████| 938/938 [03:39<00:00,  4.28it/s, loss=0.00437, train_acc=95.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Acc=95.73% | Val Acc=99.29% | Time=238.9s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/25: 100%|██████████| 938/938 [03:37<00:00,  4.32it/s, loss=0.00436, train_acc=95.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Acc=95.75% | Val Acc=99.33% | Time=236.8s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/25: 100%|██████████| 938/938 [03:37<00:00,  4.31it/s, loss=0.00438, train_acc=95.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Acc=95.79% | Val Acc=99.05% | Time=237.3s\n\nTraining completed.\nBest Validation Accuracy: 99.66%\n\n===== STUDENT MODEL SUMMARY =====\nModel: MobileNetV3-Small (0.25x)\nParameters: 118,557\nModel Size: 0.45 MB\nAvg Inference Time: 5.66 ms\n\nConfusion Matrix:\n[[2993    0    0    1    6]\n [   0 2967    0   33    0]\n [   0    0 3000    0    0]\n [   5    2    0 2992    1]\n [   3    0    0    0 2997]]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# QUANTIZATION VIA QAT FOR UNIFORM 8 BIT WITH THE BACKWARD LOSS BEING CE+KD LOSS","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# INT8 QAT + Ensemble Knowledge Distillation (NO TRADES)\n# Teachers: ResNet18, MobileNetV2, DenseNet121 (99%+)\n# Student: MobileNetV3-Small (0.25x)\n# Dataset: Rice_Image_Dataset (5 classes)\n# Platform: Kaggle GPU\n\nimport os, time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom pathlib import Path\nfrom PIL import Image\n\n# Config\nDATA_DIR = \"/kaggle/input/rice-image-dataset/Rice_Image_Dataset\"\nTEACHER_DIR = \"/kaggle/input/teacher-weights/pytorch/default/1\"\nFP32_STUDENT_CKPT = \"/kaggle/input/student-checkpoint-fp32/pytorch/default/1/best_student_mobilenetv3_025_kd.pth\"\n\nNUM_CLASSES = 5\nIMG_SIZE = 224\nBATCH_SIZE = 64\nEPOCHS = 25\nLR = 2e-4\nWEIGHT_DECAY = 1e-4\nALPHA = 0.8\nTEMPERATURE = 4.0\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(\"=\"*70)\nprint(\"INT8 QAT + ENSEMBLE KD TRAINING (NO TRADES)\")\nprint(\"Device:\", DEVICE)\nprint(\"=\"*70)\n\n# Transforms\ntrain_tfms = transforms.Compose([\n    transforms.Resize((IMG_SIZE+32, IMG_SIZE+32)),\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.75, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\nval_tfms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n# Data Split\nprint(\"[1] Building dataframe & stratified split...\")\nroot = Path(DATA_DIR)\nclasses = sorted([d.name for d in root.iterdir() if d.is_dir()])\nclass_to_idx = {c:i for i,c in enumerate(classes)}\n\npaths, labels = [], []\nfor c in classes:\n    for p in (root/c).glob('*.*'):\n        paths.append(str(p))\n        labels.append(c)\n\ndf = list(zip(paths, labels))\nidx = np.arange(len(df))\ntrain_idx, val_idx = train_test_split(idx, test_size=0.2, stratify=labels, random_state=42)\n\nclass RiceDataset(torch.utils.data.Dataset):\n    def __init__(self, indices, transform):\n        self.indices = indices\n        self.transform = transform\n    def __len__(self): return len(self.indices)\n    def __getitem__(self, i):\n        p,l = df[self.indices[i]]\n        img = Image.open(p).convert('RGB')\n        img = self.transform(img)\n        return img, class_to_idx[l]\n\ntrain_loader = DataLoader(RiceDataset(train_idx, train_tfms), batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\nval_loader   = DataLoader(RiceDataset(val_idx, val_tfms), batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n\nprint(f\"Train: {len(train_idx)} | Val: {len(val_idx)}\")\nprint(\"Classes:\", class_to_idx)\n\n# Load Teacher Models\ndef load_teacher(model, path, name):\n    print(f\"Loading teacher: {name}\")\n    ckpt = torch.load(path, map_location='cpu')\n    sd = ckpt['model_state'] if isinstance(ckpt, dict) and 'model_state' in ckpt else ckpt\n    model.load_state_dict(sd, strict=False)\n    model.eval()\n    for p in model.parameters(): p.requires_grad=False\n    return model.to(DEVICE)\n\nprint(\"[2] Loading teacher models...\")\nresnet18 = torchvision.models.resnet18(weights=None)\nresnet18.fc = nn.Linear(resnet18.fc.in_features, NUM_CLASSES)\nresnet18 = load_teacher(resnet18, f\"{TEACHER_DIR}/best_resnet18_rice.pth\", \"ResNet18\")\n\nmobilenetv2 = torchvision.models.mobilenet_v2(weights=None)\nmobilenetv2.classifier[1] = nn.Linear(mobilenetv2.classifier[1].in_features, NUM_CLASSES)\nmobilenetv2 = load_teacher(mobilenetv2, f\"{TEACHER_DIR}/best_mobilenetv2_rice_scratch.pth\", \"MobileNetV2\")\n\ndensenet121 = torchvision.models.densenet121(weights=None)\ndensenet121.classifier = nn.Linear(densenet121.classifier.in_features, NUM_CLASSES)\ndensenet121 = load_teacher(densenet121, f\"{TEACHER_DIR}/best_densenet121_rice_scratch.pth\", \"DenseNet121\")\n\nteachers = [resnet18, mobilenetv2, densenet121]\nprint(\"Teachers ready.\")\n\n# STUDENT (FP32 -> QAT)\nprint(\"[3] Loading FP32 student checkpoint...\")\nstudent = torchvision.models.mobilenet_v3_small(width_mult=0.25, weights=None)\nstudent.classifier[3] = nn.Linear(student.classifier[3].in_features, NUM_CLASSES)\nfp32_ckpt = torch.load(FP32_STUDENT_CKPT, map_location='cpu')\nstudent.load_state_dict(fp32_ckpt, strict=False)\nstudent = student.to(DEVICE)\nprint(\"FP32 student loaded.\")\n\n# KD LOSS\nclass KDLoss(nn.Module):\n    def __init__(self, alpha=0.8, T=4.0):\n        super().__init__()\n        self.alpha = alpha\n        self.T = T\n        self.ce = nn.CrossEntropyLoss(label_smoothing=0.1)\n    def forward(self, s_logits, y, t_logits):\n        ce = self.ce(s_logits, y)\n        kd = F.kl_div(\n            F.log_softmax(s_logits/self.T, dim=1),\n            F.softmax(t_logits/self.T, dim=1),\n            reduction='batchmean'\n        ) * (self.T**2)\n        return self.alpha*kd + (1-self.alpha)*ce\n\ncriterion = KDLoss(ALPHA, TEMPERATURE)\noptimizer = AdamW(student.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nscheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n\n# TRAIN / EVAL\ndef evaluate(model):\n    model.eval()\n    correct,total = 0,0\n    preds_all, labels_all = [],[]\n    with torch.no_grad():\n        for x,y in val_loader:\n            x,y = x.to(DEVICE), y.to(DEVICE)\n            out = model(x)\n            preds = out.argmax(1)\n            correct += preds.eq(y).sum().item()\n            total += y.size(0)\n            preds_all.extend(preds.cpu().numpy())\n            labels_all.extend(y.cpu().numpy())\n    return 100*correct/total, np.array(labels_all), np.array(preds_all)\n\nprint(\"[4] Starting INT8 QAT + KD training...\")\nbest_acc = 0\nbest_labels = best_preds = None\n\nfor epoch in range(EPOCHS):\n    student.train()\n    correct,total,loss_sum = 0,0,0.0\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n    for x,y in pbar:\n        x,y = x.to(DEVICE), y.to(DEVICE)\n        with torch.no_grad():\n            t_logits = sum(t(x) for t in teachers)/len(teachers)\n        s_logits = student(x)\n        loss = criterion(s_logits, y, t_logits)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        preds = s_logits.argmax(1)\n        correct += preds.eq(y).sum().item()\n        total += y.size(0)\n        loss_sum += loss.item()\n        pbar.set_postfix(train_acc=100*correct/total)\n    scheduler.step()\n    val_acc, lab, pr = evaluate(student)\n    print(f\"Epoch {epoch+1}: Train Acc={100*correct/total:.2f}% | Val Acc={val_acc:.2f}%\")\n    if val_acc > best_acc:\n        best_acc = val_acc\n        best_labels, best_preds = lab, pr\n        torch.save(student.state_dict(), \"best_student_int8_qat_kd.pth\")\n\n# FINAL REPORT\nprint(\"\\nTraining complete.\")\nprint(f\"Best INT8 Validation Accuracy: {best_acc:.2f}%\")\n\nnum_params = sum(p.numel() for p in student.parameters())\nmodel_size_mb = num_params*4/(1024**2)\nstudent.eval()\ndummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(DEVICE)\nwith torch.no_grad():\n    t0=time.time()\n    for _ in range(100): student(dummy)\n    t1=time.time()\ninf_ms = (t1-t0)/100*1000\n\nprint(\"\\n===== MODEL SUMMARY =====\")\nprint(\"Model: MobileNetV3-Small (0.25x) INT8 QAT\")\nprint(f\"Accuracy: {best_acc:.2f}%\")\nprint(f\"Parameters: {num_params:,}\")\nprint(f\"Size: {model_size_mb:.2f} MB\")\nprint(f\"Avg Inference: {inf_ms:.2f} ms ({inf_ms/1000:.4f} s)\")\n\ncm = confusion_matrix(best_labels, best_preds)\nprint(\"Confusion Matrix:\\n\", cm)\n\n# TORCHSCRIPT (PYTORCH MOBILE) \nprint(\"\\n[5] Exporting TorchScript model for PyTorch Mobile...\")\nscripted = torch.jit.script(student.cpu())\nscripted.save(\"mobilenetv3_025_int8_qat_kd.pt\")\nprint(\"Saved: mobilenetv3_025_int8_qat_kd.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T14:43:06.323758Z","iopub.execute_input":"2025-12-17T14:43:06.324032Z","iopub.status.idle":"2025-12-17T16:20:28.179528Z","shell.execute_reply.started":"2025-12-17T14:43:06.324006Z","shell.execute_reply":"2025-12-17T16:20:28.178724Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nINT8 QAT + ENSEMBLE KD TRAINING (NO TRADES)\nDevice: cuda\n======================================================================\n[1] Building dataframe & stratified split...\nTrain: 60000 | Val: 15000\nClasses: {'Arborio': 0, 'Basmati': 1, 'Ipsala': 2, 'Jasmine': 3, 'Karacadag': 4}\n[2] Loading teacher models...\nLoading teacher: ResNet18\nLoading teacher: MobileNetV2\nLoading teacher: DenseNet121\nTeachers ready.\n[3] Loading FP32 student checkpoint...\nFP32 student loaded.\n[4] Starting INT8 QAT + KD training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/25: 100%|██████████| 938/938 [03:57<00:00,  3.95it/s, train_acc=97.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Acc=97.06% | Val Acc=99.15%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/25: 100%|██████████| 938/938 [03:38<00:00,  4.29it/s, train_acc=97.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Acc=97.08% | Val Acc=99.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/25: 100%|██████████| 938/938 [03:29<00:00,  4.47it/s, train_acc=97]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Acc=97.04% | Val Acc=93.36%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/25: 100%|██████████| 938/938 [03:29<00:00,  4.47it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Acc=97.19% | Val Acc=99.90%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/25: 100%|██████████| 938/938 [03:32<00:00,  4.41it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Acc=97.16% | Val Acc=96.22%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/25: 100%|██████████| 938/938 [03:32<00:00,  4.42it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Acc=97.16% | Val Acc=98.35%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/25: 100%|██████████| 938/938 [03:30<00:00,  4.46it/s, train_acc=97.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Acc=97.09% | Val Acc=90.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/25: 100%|██████████| 938/938 [03:34<00:00,  4.38it/s, train_acc=97.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Acc=97.13% | Val Acc=99.89%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/25: 100%|██████████| 938/938 [03:30<00:00,  4.45it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Acc=97.16% | Val Acc=98.93%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/25: 100%|██████████| 938/938 [03:31<00:00,  4.43it/s, train_acc=97.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Acc=97.09% | Val Acc=99.86%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/25: 100%|██████████| 938/938 [03:38<00:00,  4.29it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Acc=97.24% | Val Acc=99.77%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/25: 100%|██████████| 938/938 [03:32<00:00,  4.42it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Acc=97.16% | Val Acc=99.25%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/25: 100%|██████████| 938/938 [03:34<00:00,  4.38it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Acc=97.23% | Val Acc=98.81%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/25: 100%|██████████| 938/938 [03:34<00:00,  4.38it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Acc=97.17% | Val Acc=99.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/25: 100%|██████████| 938/938 [03:34<00:00,  4.38it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Acc=97.19% | Val Acc=99.71%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/25: 100%|██████████| 938/938 [03:31<00:00,  4.44it/s, train_acc=97.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Acc=97.27% | Val Acc=99.78%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/25: 100%|██████████| 938/938 [03:29<00:00,  4.47it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Acc=97.17% | Val Acc=99.89%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/25: 100%|██████████| 938/938 [03:30<00:00,  4.46it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Acc=97.23% | Val Acc=99.68%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/25: 100%|██████████| 938/938 [03:30<00:00,  4.46it/s, train_acc=97.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Acc=97.28% | Val Acc=99.87%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/25: 100%|██████████| 938/938 [03:30<00:00,  4.45it/s, train_acc=97.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Acc=97.31% | Val Acc=99.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/25: 100%|██████████| 938/938 [03:28<00:00,  4.50it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Acc=97.22% | Val Acc=99.63%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/25: 100%|██████████| 938/938 [03:29<00:00,  4.47it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Acc=97.18% | Val Acc=99.62%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/25: 100%|██████████| 938/938 [03:30<00:00,  4.46it/s, train_acc=97.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Acc=97.14% | Val Acc=99.59%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/25: 100%|██████████| 938/938 [03:31<00:00,  4.43it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Acc=97.19% | Val Acc=99.65%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/25: 100%|██████████| 938/938 [03:33<00:00,  4.39it/s, train_acc=97.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Acc=97.24% | Val Acc=99.59%\n\nTraining complete.\nBest INT8 Validation Accuracy: 99.91%\n\n===== MODEL SUMMARY =====\nModel: MobileNetV3-Small (0.25x) INT8 QAT\nAccuracy: 99.91%\nParameters: 118,557\nSize: 0.45 MB\nAvg Inference: 5.52 ms (0.0055 s)\nConfusion Matrix:\n [[2997    0    0    1    2]\n [   0 2998    0    2    0]\n [   0    0 3000    0    0]\n [   0    4    0 2996    0]\n [   4    0    0    0 2996]]\n\n[5] Exporting TorchScript model for PyTorch Mobile...\nSaved: mobilenetv3_025_int8_qat_kd.pt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torchvision\n\nNUM_CLASSES = 5\nIMG_SIZE = 224\n\nmodel = torchvision.models.mobilenet_v3_small(width_mult=0.25)\nmodel.classifier[3] = torch.nn.Linear(\n    model.classifier[3].in_features, NUM_CLASSES\n)\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_student_int8_qat_kd.pth\", map_location=\"cpu\"))\nmodel.eval()\n\ndummy = torch.randn(1, 3, IMG_SIZE, IMG_SIZE)\n\ntorch.onnx.export(\n    model,\n    dummy,\n    \"student_fp32_qat.onnx\",\n    input_names=[\"input\"],\n    output_names=[\"logits\"],\n    opset_version=13,\n    do_constant_folding=True,\n    dynamic_axes={\"input\": {0: \"batch\"}}\n)\n\nprint(\"Exported student_fp32_qat.onnx\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T16:33:58.338566Z","iopub.execute_input":"2025-12-17T16:33:58.339841Z","iopub.status.idle":"2025-12-17T16:33:59.145193Z","shell.execute_reply.started":"2025-12-17T16:33:58.339804Z","shell.execute_reply":"2025-12-17T16:33:59.144349Z"}},"outputs":[{"name":"stdout","text":"Exported student_fp32_qat.onnx\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install onnxruntime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:18:14.993296Z","iopub.execute_input":"2025-12-18T05:18:14.994043Z","iopub.status.idle":"2025-12-18T05:18:20.789434Z","shell.execute_reply.started":"2025-12-18T05:18:14.994013Z","shell.execute_reply":"2025-12-18T05:18:20.788684Z"}},"outputs":[{"name":"stdout","text":"Collecting onnxruntime\n  Downloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nCollecting coloredlogs (from onnxruntime)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\nRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (6.33.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (2.4.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->onnxruntime) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->onnxruntime) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->onnxruntime) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.6->onnxruntime) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.6->onnxruntime) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.6->onnxruntime) (2024.2.0)\nDownloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from onnxruntime.quantization import quantize_dynamic, QuantType\n\nquantize_dynamic(\n    model_input=\"/kaggle/working/student_fp32_qat.onnx\",\n    model_output=\"/kaggle/working/student_int8.onnx\",\n    weight_type=QuantType.QInt8\n)\n\nprint(\"INT8 ONNX model saved as student_int8.onnx\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T16:36:13.653812Z","iopub.execute_input":"2025-12-17T16:36:13.654419Z","iopub.status.idle":"2025-12-17T16:36:13.746432Z","shell.execute_reply.started":"2025-12-17T16:36:13.654393Z","shell.execute_reply":"2025-12-17T16:36:13.745631Z"}},"outputs":[{"name":"stderr","text":"WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n","output_type":"stream"},{"name":"stdout","text":"INT8 ONNX model saved as student_int8.onnx\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# STATIC INT8 ONNX QUANTIZATION + EVALUATION\n\n# IMPORTS\nimport os\nimport time\nimport shutil\nimport random\nimport numpy as np\nimport onnx\nimport onnxruntime as ort\n\nfrom onnxruntime.quantization import (\n    quantize_static,\n    QuantType,\n    CalibrationDataReader\n)\n\nfrom pathlib import Path\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\n\n# CONFIG\nFP32_ONNX_INPUT = \"/kaggle/input/student-onnx/onnx/default/1/student_fp32_qat.onnx\"\nWORK_FP32_ONNX  = \"/kaggle/working/student_fp32_qat.onnx\"\nINT8_ONNX_PATH  = \"/kaggle/working/student_int8_static.onnx\"\n\nDATA_DIR = \"/kaggle/input/rice-image-dataset/Rice_Image_Dataset\"\n\nIMG_SIZE = 224\nBATCH_SIZE = 32\nNUM_CLASSES = 5\nCALIB_SAMPLES = 200 \n\nCLASSES = [\"Arborio\", \"Basmati\", \"Ipsala\", \"Jasmine\", \"Karacadag\"]\n\nprint(\"=\"*70)\nprint(\"STATIC INT8 ONNX CONVERSION + EVALUATION\")\nprint(\"=\"*70)\n\n# COPY FP32 ONNX TO WRITABLE DIRECTORY\nprint(\"[0] Copying FP32 ONNX to /kaggle/working...\")\nshutil.copy(FP32_ONNX_INPUT, WORK_FP32_ONNX)\nprint(\"FP32 ONNX ready:\", WORK_FP32_ONNX)\n\n# CALIBRATION DATA READER\nprint(\"[1] Preparing calibration data reader...\")\n\nclass RiceCalibrationReader(CalibrationDataReader):\n    def __init__(self, num_samples):\n        self.transform = transforms.Compose([\n            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n        paths = []\n        for c in CLASSES:\n            paths.extend(list((Path(DATA_DIR) / c).glob(\"*.*\")))\n\n        random.shuffle(paths)\n        random.seed(42)\n        paths = paths[:num_samples]\n\n        self.data = []\n        for p in paths:\n            img = Image.open(p).convert(\"RGB\")\n            x = self.transform(img).unsqueeze(0).numpy()\n            self.data.append({\"input\": x})\n\n        self.iterator = iter(self.data)\n\n    def get_next(self):\n        return next(self.iterator, None)\n\n\n# STATIC INT8 QUANTIZATION\nprint(\"[2] Converting FP32 QAT ONNX → Static INT8 ONNX...\")\n\nquantize_static(\n    model_input=WORK_FP32_ONNX,\n    model_output=INT8_ONNX_PATH,\n    calibration_data_reader=RiceCalibrationReader(CALIB_SAMPLES),\n    weight_type=QuantType.QInt8,\n    activation_type=QuantType.QInt8\n)\n\nprint(\"INT8 model saved:\", INT8_ONNX_PATH)\n\n# MODEL SIZE & QUANT INFO\nsize_mb = os.path.getsize(INT8_ONNX_PATH) / (1024 ** 2)\nprint(f\"Model size: {size_mb:.2f} MB\")\n\nonnx_model = onnx.load(INT8_ONNX_PATH)\nint8_tensors = sum(\n    1 for t in onnx_model.graph.initializer\n    if t.data_type == onnx.TensorProto.INT8\n)\nfp32_tensors = sum(\n    1 for t in onnx_model.graph.initializer\n    if t.data_type == onnx.TensorProto.FLOAT\n)\n\nprint(f\"INT8 tensors: {int8_tensors}\")\nprint(f\"FP32 tensors: {fp32_tensors}\")\nprint(\"Quantization scheme: Static INT8\")\n\n# VALIDATION DATASET\nprint(\"[3] Preparing validation dataset...\")\n\npaths, labels = [], []\nfor c in CLASSES:\n    for p in (Path(DATA_DIR) / c).glob(\"*.*\"):\n        paths.append(str(p))\n        labels.append(CLASSES.index(c))\n\nidx = np.arange(len(paths))\n_, val_idx = train_test_split(\n    idx, test_size=0.2, stratify=labels, random_state=42\n)\n\nclass RiceValDataset(Dataset):\n    def __init__(self, indices):\n        self.indices = indices\n        self.transform = transforms.Compose([\n            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, i):\n        idx = self.indices[i]\n        img = Image.open(paths[idx]).convert(\"RGB\")\n        return self.transform(img).numpy(), labels[idx]\n\nval_loader = DataLoader(\n    RiceValDataset(val_idx),\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\nprint(f\"Validation samples: {len(val_idx)}\")\n\n# ONNX RUNTIME SESSION\nprint(\"[4] Creating ONNX Runtime session...\")\nsess = ort.InferenceSession(\n    INT8_ONNX_PATH,\n    providers=[\"CPUExecutionProvider\"]\n)\ninput_name = sess.get_inputs()[0].name\n\n\n# ACCURACY EVALUATION\nprint(\"[5] Running accuracy evaluation...\")\n\ncorrect = 0\ntotal = 0\nall_preds = []\nall_labels = []\n\nfor x, y in val_loader:\n    logits = sess.run(None, {input_name: x.numpy()})[0]\n    preds = logits.argmax(axis=1)\n\n    correct += (preds == y.numpy()).sum()\n    total += y.size(0)\n\n    all_preds.extend(preds.tolist())\n    all_labels.extend(y.numpy().tolist())\n\nacc = 100.0 * correct / total\n\n# LATENCY TEST\ndummy = np.random.randn(1, 3, IMG_SIZE, IMG_SIZE).astype(np.float32)\n\n# Warm-up\nfor _ in range(10):\n    sess.run(None, {input_name: dummy})\n\nt0 = time.time()\nfor _ in range(100):\n    sess.run(None, {input_name: dummy})\nt1 = time.time()\n\navg_ms = (t1 - t0) / 100 * 1000\n\n# FINAL REPORT\nprint(\"\\n\" + \"=\"*30 + \" FINAL REPORT \" + \"=\"*30)\nprint(\"Model: MobileNetV3-Small – Static INT8\")\nprint(f\"Calibration samples: {CALIB_SAMPLES}\")\nprint(f\"Accuracy: {acc:.2f}%\")\nprint(f\"Model size: {size_mb:.2f} MB\")\nprint(f\"Avg inference latency: {avg_ms:.2f} ms\")\n\ncm = confusion_matrix(all_labels, all_preds)\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:50:47.499854Z","iopub.execute_input":"2025-12-18T05:50:47.500206Z","iopub.status.idle":"2025-12-18T05:52:13.204831Z","shell.execute_reply.started":"2025-12-18T05:50:47.500185Z","shell.execute_reply":"2025-12-18T05:52:13.204146Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nSTATIC INT8 ONNX CONVERSION + EVALUATION\n======================================================================\n[0] Copying FP32 ONNX to /kaggle/working...\nFP32 ONNX ready: /kaggle/working/student_fp32_qat.onnx\n[1] Preparing calibration data reader...\n[2] Converting FP32 QAT ONNX → Static INT8 ONNX...\n","output_type":"stream"},{"name":"stderr","text":"WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \nWARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n","output_type":"stream"},{"name":"stdout","text":"INT8 model saved: /kaggle/working/student_int8_static.onnx\nModel size: 0.31 MB\nINT8 tensors: 237\nFP32 tensors: 237\nQuantization scheme: Static INT8\n[3] Preparing validation dataset...\nValidation samples: 15000\n[4] Creating ONNX Runtime session...\n[5] Running accuracy evaluation...\n\n============================== FINAL REPORT ==============================\nModel: MobileNetV3-Small – Static INT8\nCalibration samples: 200\nAccuracy: 98.07%\nModel size: 0.31 MB\nAvg inference latency: 3.95 ms\n\nConfusion Matrix:\n[[2990    0    1    2    7]\n [   0 2998    0    2    0]\n [   0    0 2999    1    0]\n [  10  209    0 2730   51]\n [   5    2    0    0 2993]]\n================================================================================\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import onnxruntime as ort\nimport numpy as np\nimport time\nfrom PIL import Image\nfrom torchvision import transforms\n\n# CONFIG\nMODEL_PATH = \"/kaggle/working/student_int8_static.onnx\"\nIMAGE_PATH = \"/kaggle/input/rice-image-dataset/Rice_Image_Dataset/Arborio/Arborio (1000).jpg\"\n\nCLASSES = [\"Arborio\", \"Basmati\", \"Ipsala\", \"Jasmine\", \"Karacadag\"]\nIMG_SIZE = 224\nNUM_RUNS = 100    \n\ndef softmax(x):\n    x = x - np.max(x, axis=1, keepdims=True)\n    exp_x = np.exp(x)\n    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n\n# PREPROCESS\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\nimg = Image.open(IMAGE_PATH).convert(\"RGB\")\nx = transform(img).unsqueeze(0).numpy() \n\n# ONNX RUNTIME\nprint(\"Loading ONNX Runtime session...\")\nsess = ort.InferenceSession(\n    MODEL_PATH,\n    providers=[\"CPUExecutionProvider\"]\n)\n\ninput_name = sess.get_inputs()[0].name\noutput_name = sess.get_outputs()[0].name\n\n# WARM-UP\nfor _ in range(10):\n    sess.run([output_name], {input_name: x})\n\n# TIMING\nstart = time.time()\nfor _ in range(NUM_RUNS):\n    logits = sess.run([output_name], {input_name: x})[0]\nend = time.time()\n\navg_time_sec = (end - start) / NUM_RUNS\navg_time_ms = avg_time_sec * 1000\n\n# PREDICTION\nprobs = softmax(logits)[0]\npred_id = probs.argmax()\npred_class = CLASSES[pred_id]\nconfidence = probs[pred_id] * 100\n\n# OUTPUT\nprint(\"Prediction:\", pred_class)\nprint(f\"Confidence: {confidence:.2f}%\")\nprint(f\"Average inference time: {avg_time_ms:.2f} ms ({avg_time_sec:.4f} s)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:52:34.591705Z","iopub.execute_input":"2025-12-18T05:52:34.592108Z","iopub.status.idle":"2025-12-18T05:52:35.006161Z","shell.execute_reply.started":"2025-12-18T05:52:34.592085Z","shell.execute_reply":"2025-12-18T05:52:35.005475Z"}},"outputs":[{"name":"stdout","text":"Loading ONNX Runtime session...\nPrediction: Arborio\nConfidence: 98.25%\nAverage inference time: 2.99 ms (0.0030 s)\n","output_type":"stream"}],"execution_count":15}]}
